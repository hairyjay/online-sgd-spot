# An unique identifier for the head node and workers of this cluster.
cluster_name: spot

# The minimum number of workers nodes to launch in addition to the head
# node. This number should be >= 0.
#min_workers: 65

# The maximum number of workers nodes to launch in addition to the head
# node. This takes precedence over min_workers.
max_workers: 65

#initial_workers: 65

# The autoscaler will scale up the cluster to this target fraction of resource
# usage. For example, if a cluster of 10 nodes is 100% busy and
# target_utilization is 0.8, it would resize the cluster to 13. This fraction
# can be decreased to increase the aggressiveness of upscaling.
# This value must be less than 1.0 for scaling to happen.
#target_utilization_fraction: 0.8
upscaling_speed: 1.0

# This executes all commands on all nodes in the docker container,
# and opens all the necessary ports to support the Ray cluster.
# Empty string means disabled.
docker:
    image: "rayproject/ray-ml:latest-gpu" # You can change this to latest-cpu if you don't need GPU support and want a faster startup
    # image: rayproject/ray:latest-gpu   # use this one if you don't need ML dependencies, it's faster to pull
    container_name: "ray_container"
    # If true, pulls latest version of image. Otherwise, `docker run` will only pull the image
    # if no cached version is present.
    pull_before_run: True
    run_options: []  # Extra options to pass into "docker run"

# If a node is idle for this many minutes, it will be removed.
idle_timeout_minutes: 5

# Cloud-provider specific configuration.
provider:
    type: aws
    region: us-east-1
    # Availability zone(s), comma-separated, that nodes may be launched in.
    # Nodes are currently spread between zones by a round-robin approach,
    # however this implementation detail should not be relied upon.
    availability_zone: us-east-1a, us-east-1b, us-east-1c, us-east-1d, us-east-1e, us-east-1f

# How Ray will authenticate with newly launched nodes.
auth:
    ssh_user: ubuntu
    # By default Ray creates a new private keypair, but you can also use your own.
    # If you do so, make sure to also set "KeyName" in the head and worker node
    # configurations below.
    #ssh_private_key: ../hhj.pem





available_node_types:
    cpu_ps:
        node_config:
            InstanceType: m4.xlarge
            ImageId: ami-0b70285e5215b80eb # Amazon Deep Learning AMI (Ubuntu) Version 42.1
            #KeyName: hhj

            # Set primary volume to 120 GiB
            BlockDeviceMappings:
                - DeviceName: /dev/sda1
                  Ebs:
                      VolumeSize: 140

    gpu_ts:
        node_config:
            InstanceType: g3.4xlarge
            ImageId: ami-0b70285e5215b80eb # Amazon Deep Learning AMI (Ubuntu) Version 42.1
            #KeyName: hhj

            # Set primary volume to 120 GiB
            BlockDeviceMappings:
                - DeviceName: /dev/sda1
                  Ebs:
                      VolumeSize: 140
        min_workers: 1
        max_workers: 1

    cpu_w:
        node_config:
            InstanceType: c5.large
            ImageId: ami-0b70285e5215b80eb # Amazon Deep Learning AMI (Ubuntu) Version 42.1
            #KeyName: hhj

            # Set primary volume to 25 GiB
            BlockDeviceMappings:
                - DeviceName: /dev/sda1
                  Ebs:
                      VolumeSize: 140
        min_workers: 64
        max_workers: 64

head_node_type: cpu_ps

# Files or directories to copy to the head and worker nodes. The format is a
# dictionary from REMOTE_PATH: LOCAL_PATH, e.g.
file_mounts: {
   "~/test": "./test",
   "~/main": "./main",
   "~/price-trace": "./price-trace",
   "~/spot_aws/data": "./datasets",
   # "~/spot/": "/Users/jingjing/Desktop/cmu/18fall-research/ML-on-the-Cheap/DistSGDonRay/",
   #"~/spot_aws/": "/Users/jianyuw1/Downloads/ML-on-the-Cheap-master/spot_aws/",
   # "~/spot/": "/Users/jingjing/Desktop/cmu/18fall-research/ML-on-the-Cheap/DistSGDonRay/examples/parameter_server/sync_parameter_server.py",
#    "/path2/on/remote/machine": "/path2/on/local/machine",
}

# List of shell commands to run to set up nodes.
setup_commands:
    - export LC_ALL=C.UTF-8
    - export LANG=C.UTF-8

# Custom commands that will be run on the head node after common setup.
head_setup_commands: []

# Custom commands that will be run on worker nodes after common setup.
worker_setup_commands: []

# Command to start ray on the head node. You don't need to change this.
head_start_ray_commands:
    - ray stop
    - ulimit -n 65536; ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml

# Command to start ray on worker nodes. You don't need to change this.
worker_start_ray_commands:
    - ray stop
    - ulimit -n 65536; ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076
